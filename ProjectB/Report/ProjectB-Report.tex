\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Project B: Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks
}
\maketitle

\begin{abstract}
This paper is a report for Project A of ECE1512 2022W, University of Toronto.
In this paper, we introduce two tasks assigned to us in detail, including the implementation and evaluation of KD based on two couples of Teacher-Student Models.
For the 
\end{abstract}

\section{Task 1: Knowledge Distillation in MNIST dataset}
In this task, we basically implement the load \& preprocess of dataset, model construction, training and evaluation for teacher and student models using our own training functions.
In addition, we implement the Early-Stopping Knowledge Distillation as the improving algorithm.
\subsection{Question 1}

In this section, we read the paper of Geoffrey Hinton\cite{b6}, and answer the assigned questions.

\subsubsection{SubQuestion a}
The purpose of using Knowledge Distillation is to compress the knowledge in an ensemble to a single model which is much easier to deploy.

\subsubsection{SubQuestion b}
In the paper, what knowledge is transferred from the teacher model to the student model?
\subsubsection{SubQuestion c}
What is the temperature hyper parameter T? Why do we use it when transferring knowledge
from one model to another? What effect does the temperature hyper parameter have in KD?
\subsubsection{SubQuestion d}
Explain in detail the loss functions on which the teacher and student model are trained in
this paper. How does the task balance parameter affect student learning? 
\subsubsection{SubQuestion e}
Can we look at the KD as a regularization technique, here? Explain your rationale.
\subsection{Question 2}
\subsection{Question 3}
\subsection{Question 4}
\subsection{Question 5}
\subsection{Question 6}
\subsection{Question 7}
\subsection{Question 8}
\subsection{Question 9}
\subsection{Question 10}
\subsection{Question 11}
\subsection{Question 12}
\subsection{Question 13}

\section{Task 2: Knowledge Distillation in MHIST dataset}

\subsection{Question 1}
\subsubsection{SubQuestion a} How can we adapt these models for the MHIST dataset using transfer learning? Talk about the Feature Extraction and Fine-Tuning processes during transfer learning.\\

The core idea of Transfer learning is consists of taking features learned on one problem, and leveraging them on a new, similar problem.\cite{keras}
Feature Extraction is 
\\
Fine tuning
\cite{blog}
\subsubsection{SubQuestion b} What is a residual block in ResNet architectures?
\subsubsection{SubQuestion c} What are the differences between the ResNetV1 and ResNetV2 architectures?
\subsubsection{SubQuestion d} What are the differences between the MobileNetV1 and MobileNetV2 architectures?
\subsubsection{SubQuestion e} How can ResNet architectures, regardless of model depth, overcome the vanishing gradient problem?
\subsubsection{SubQuestion f} Is MobileNetV2 a lightweight model? Why?
\subsection{Question 2} Explain the effect of transfer learning and knowledge distillation in the performance of the student model. Do pre-trained weights help the teacher and student models perform well on the MHIST dataset? Does knowledge transfer from the teacher to the student model increase the student’s performance?

\begin{thebibliography}{00}


\bibitem{b2} Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4794–4802, 2019. http://openaccess.thecvf.com/content\_ICCV\_2019/papers/\\Cho\_On\_the\_Efficacy\_of\_Knowledge\_Distillation\_\\ICCV\_2019\_paper.pdf.
\bibitem{b6} Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. https://arxiv.org/abs/1503.02531.


%%%%%%%%%%
\bibitem{keras} Transfer learning \& fine-tuning, Keras developer guide, 2020, https://keras.io/guides/transfer\_learning.
\bibitem{blog} Anusua Trivedi, Deep Learning Part 2: Transfer Learning and Fine-tuning Deep Convolutional Neural Networks, Revolutions. https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html.
\end{thebibliography}


\end{document}
